["Alfred P. Sloan Research Fellow", "CIFAR", "Canada Research Chair", "University of British Columbia", "Laboratory for Computational Intelligence", "Simon Fraser University", "Ecole Normale Superieure", "INRIA SIERRA", "University of British Columbia", "Scientific Computing Lab", "University of British Columbia", "Laboratory for Computational Intelligence", "Siemens Medical Solutions", "Computer-Assisted Diagnosis and Therapy Group", "University of Alberta", "Alberta Ingenuity Center for Machine Learning", "Superlinear Convergence", "Linear Convergence of Gradient", "the Polyak-Lojasiewicz Condition", "Linear Convergence", "SIOPT", "Tractable Big Data", "Structure Learning", "Hessian-Free Newton Methods", "Linear Convergence and Support Vector Identification of Sequential Minimal Optimization", "BayesOpt", "Linear Convergence of Gradient", "the Polyak-Lojasiewicz Condition", "Simons Institute", "Hessian-Free Newton Methods)", "Convex Structure Learning", "Matlab", "CPSC", "Stochastic Convex Optimization Methods in Machine Learning", "MLSS", "Convex Optimization", "SIOPT", "DLSS", "Convex Optimization", "Modern Convex Optimization Methods for Large-Scale Empirical Risk Minimization", "Convex Optimization for Big Data", "MLSS", "Convex Optimization", "Convex Optimization for Big Data", "Linear Algebra", "UBC Machine Learning Lab", "Borealis AI", "Oracle", "Cornell", "UrtheCast", "Stanford", "MILA", "Google", "UBC Machine Learning Reading Group", "ML Theory Reading Group", "the Hinge Loss Convergence Rates of Stochastic Optimization Algorithms Generalized Interventional Potentials Introduction", "Cauchy"]